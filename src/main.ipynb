{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Compute dtype: float16\n",
      "Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics, model_selection\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50 as Engine\n",
    "\n",
    "from albumentations import *\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "num_gpus = len(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(num_gpus, \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test shape = (10982, 6)\n",
      "     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n",
      "0  ISIC_0052060  IP_3579794    male        70.0                           NaN   \n",
      "1  ISIC_0052349  IP_7782715    male        40.0               lower extremity   \n",
      "2  ISIC_0058510  IP_7960270  female        55.0                         torso   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "\n",
      "train shape = (33126, 8)\n",
      "     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n",
      "0  ISIC_2637011  IP_7279968    male        45.0                     head/neck   \n",
      "1  ISIC_0015719  IP_3075186  female        45.0               upper extremity   \n",
      "2  ISIC_0052212  IP_2842074  female        50.0               lower extremity   \n",
      "\n",
      "  diagnosis benign_malignant  target  \n",
      "0   unknown           benign       0  \n",
      "1   unknown           benign       0  \n",
      "2     nevus           benign       0  \n"
     ]
    }
   ],
   "source": [
    "input_path = '../input/siim-isic-melanoma-classification/'\n",
    "train_data = pd.read_csv(input_path + 'train.csv')\n",
    "test_data = pd.read_csv(input_path + 'test.csv')\n",
    "submission_data = pd.read_csv(input_path + 'sample_submission.csv')\n",
    "test_data['target'] = 0\n",
    "print(\"test shape =\", test_data.shape)\n",
    "print(test_data.head(3))\n",
    "print(\"\\ntrain shape =\", train_data.shape)\n",
    "print(train_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = (\n",
    "    Compose([\n",
    "        OneOf([\n",
    "            ShiftScaleRotate(\n",
    "                shift_limit=0.0625,\n",
    "                scale_limit=0.1,\n",
    "                rotate_limit=90,\n",
    "                p=0.5),\n",
    "            ElasticTransform(\n",
    "                alpha=601,\n",
    "                sigma=20,\n",
    "                alpha_affine=10,\n",
    "                p=0.5),\n",
    "            GridDistortion(\n",
    "                num_steps=3,\n",
    "                distort_limit=0.5,\n",
    "                p=0.5),\n",
    "            NoOp()\n",
    "        ]),\n",
    "        OneOf([\n",
    "            RandomBrightnessContrast(\n",
    "                brightness_limit=0.15,\n",
    "                contrast_limit=0.15,\n",
    "                p=0.5),\n",
    "            RandomGamma(\n",
    "                gamma_limit=(85, 115),\n",
    "                p=0.5),\n",
    "            NoOp()\n",
    "        ]),\n",
    "        OneOf([\n",
    "            RGBShift(\n",
    "                r_shift_limit=(-15, 15),\n",
    "                g_shift_limit=(-15, 15),\n",
    "                b_shift_limit=(-15, 15),\n",
    "                p=0.5),\n",
    "            HueSaturationValue(\n",
    "                hue_shift_limit=(-15, 15),\n",
    "                sat_shift_limit=(-25, 25),\n",
    "                val_shift_limit=(-15, 15),\n",
    "                p=0.5),\n",
    "            NoOp()\n",
    "        ]),\n",
    "        RandomRotate90(\n",
    "            p=0.5),\n",
    "        Flip(\n",
    "            p=0.5),\n",
    "        Transpose(\n",
    "            p=0.5),\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "def tf_map_decorator(func):\n",
    "    \"\"\"Decorate python functions with this \n",
    "    decorator: Enables arbitrary python \n",
    "    code to work in the TF graph\n",
    "    \"\"\"\n",
    "    def wrapper(*args):\n",
    "        return tf.py_function(\n",
    "            func=func,\n",
    "            inp=[*args],\n",
    "            Tout=[a.dtype for a in args]\n",
    "        )\n",
    "    return wrapper\n",
    "\n",
    "@tf_map_decorator\n",
    "def _apply_augmentation(image, target):\n",
    "    \"\"\"Using albumentations augmentations wrapped in\n",
    "    tf.py_functions is very convenient. However, for \n",
    "    better performance, consider using TF operations \n",
    "    instead to augment the image data\n",
    "    \"\"\"\n",
    "    image = augmentor(image=image.numpy())['image']\n",
    "    return image, target\n",
    "\n",
    "def _read_input(path, target, target_size):\n",
    "    \"\"\"Reads, decodes and resizes the image.\n",
    "    'target' is also passed to the function for \n",
    "    convenience (to work well with dataset.map(..))\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize_with_pad(image, *target_size[:-1], method='bilinear')\n",
    "    image = tf.cast(image, dtype=tf.uint8)\n",
    "    return image, target\n",
    "\n",
    "def _preprocess_input(image, target):\n",
    "    \"\"\"Simple preprocessing of the input,\n",
    "    before being fed to the neural network\n",
    "    \"\"\"\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    target = tf.cast(target, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, target\n",
    "\n",
    "def create_dataset(path, \n",
    "                   dataframe, \n",
    "                   input_shape,\n",
    "                   batch_size,\n",
    "                   augment=False,\n",
    "                   shuffle=False,\n",
    "                   cache=False,):\n",
    "    \n",
    "    if cache:\n",
    "        if not(os.path.isdir('tmp/')):\n",
    "            os.mkdir('tmp/')\n",
    "        else:\n",
    "            files = glob.glob('tmp/*')\n",
    "            for file in files:\n",
    "                os.remove(file)\n",
    "        \n",
    "        cache_path = 'tmp/' + path.split('/')[-1]\n",
    "    \n",
    "    if shuffle:\n",
    "        dataframe = dataframe.sample(frac=1.0, replace=False)\n",
    "    \n",
    "    image_paths = path + dataframe.image_name + '.jpg'\n",
    "    targets = np.expand_dims(dataframe.target, -1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, targets))\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: (\n",
    "            _read_input(x, y, target_size=input_shape)\n",
    "        ), tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    if cache:\n",
    "        dataset = dataset.cache(cache_path)\n",
    "    \n",
    "    if augment:\n",
    "        dataset = dataset.map(_apply_augmentation, tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(_preprocess_input, tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, engine, input_shape, units,\n",
    "                 dropout, activation, weights=None):\n",
    "\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.engine = engine(\n",
    "            include_top=False,\n",
    "            input_shape=input_shape,\n",
    "            weights=weights)\n",
    "\n",
    "        self.pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "        self.head = tf.keras.Sequential()\n",
    "        for drop, unit, actv in zip(dropout, units, activation):\n",
    "            self.head.add(tf.keras.layers.Dropout(drop))\n",
    "            self.head.add(tf.keras.layers.Dense(unit, actv, dtype='float32'))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.engine(inputs)\n",
    "        x = self.pool(x)\n",
    "        return self.head(x)\n",
    "    \n",
    "    \n",
    "class DistributedModel:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 keras_model,\n",
    "                 batch_size=8,\n",
    "                 optimizer=None, \n",
    "                 strategy=None,\n",
    "                 mixed_precision=False, \n",
    "                 label_smoothing=0.0):\n",
    "        \n",
    "        self.keras_model = keras_model\n",
    "        self.global_batch_size = batch_size\n",
    "        self.mixed_precision = mixed_precision\n",
    "        self.optimizer = optimizer\n",
    "        self.strategy = strategy\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        if self.optimizer and self.mixed_precision:\n",
    "            self.optimizer = \\\n",
    "                tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n",
    "                    optimizer, loss_scale='dynamic')\n",
    "                \n",
    "        if self.strategy:\n",
    "            self.global_batch_size *= self.strategy.num_replicas_in_sync\n",
    "            \n",
    "        if not(os.path.isdir('output/weights')):\n",
    "            os.makedirs('output/weights')      \n",
    "    \n",
    "    \n",
    "    def _compute_loss(self, labels, logits):\n",
    "        per_example_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=labels, logits=logits) \n",
    "        return tf.nn.compute_average_loss(\n",
    "            per_example_loss, global_batch_size=self.global_batch_size)\n",
    "        \n",
    "        \n",
    "    def _train_step(self, inputs):\n",
    "        \n",
    "        images, labels = inputs\n",
    "        \n",
    "        if self.label_smoothing:\n",
    "            labels = (\n",
    "                labels * (1 - self.label_smoothing)\n",
    "                + 0.5 * self.label_smoothing\n",
    "            )\n",
    "            \n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.keras_model(images, training=True)\n",
    "            loss = self._compute_loss(labels, logits)\n",
    "            if self.mixed_precision:\n",
    "                scaled_loss = self.optimizer.get_scaled_loss(loss)\n",
    "                \n",
    "        if self.mixed_precision:\n",
    "            scaled_gradients = tape.gradient(\n",
    "                scaled_loss, self.keras_model.trainable_variables)\n",
    "            gradients = self.optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "        else:\n",
    "            gradients = tape.gradient(loss, self.keras_model.trainable_variables)\n",
    "            \n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.keras_model.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def _predict_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        logits = self.keras_model(images, training=False)\n",
    "        return tf.math.sigmoid(logits), labels\n",
    "    \n",
    "    \n",
    "    def predict(self, test_ds, weights, tta, strategy):\n",
    "        \n",
    "        @tf.function\n",
    "        def distributed_predict_step(dist_inputs):\n",
    "            preds, trues = strategy.run(self._predict_step, args=(dist_inputs,))\n",
    "            if tf.is_tensor(preds):\n",
    "                return [preds], [trues]\n",
    "            else:\n",
    "                return preds.values, trues.values\n",
    "        \n",
    "        test_dist_ds = strategy.experimental_distribute_dataset(test_ds)\n",
    "        test_dist_ds = tqdm.tqdm(test_dist_ds)\n",
    "        self.keras_model.load_weights(weights)\n",
    "        \n",
    "        preds_accum = np.zeros([0, 1], dtype=np.float32)\n",
    "        trues_accum = np.zeros([0, 1], dtype=np.float32)\n",
    "        for inputs in test_dist_ds.repeat(tta):\n",
    "            preds, trues = distributed_predict_step(inputs)\n",
    "        \n",
    "            for pred, true in zip(preds, trues):\n",
    "                preds_accum = np.concatenate([preds_accum, pred.numpy()], axis=0)\n",
    "                trues_accum = np.concatenate([trues_accum, true.numpy()], axis=0)\n",
    "        \n",
    "        preds_accum = np.array(preds_accum).reshape((tta, -1))\n",
    "        trues_accum = np.array(trues_accum).reshape((tta, -1))\n",
    "        \n",
    "        return preds_accum, trues_accum\n",
    "            \n",
    "        \n",
    "    def fit_and_predict(self, fold, epochs, train_ds, valid_ds, test_ds, strategy):\n",
    "    \n",
    "        @tf.function\n",
    "        def distributed_train_step(dist_inputs):\n",
    "            per_replica_loss = strategy.run(self._train_step, args=(dist_inputs,))\n",
    "            return strategy.reduce(\n",
    "                tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_predict_step(dist_inputs):\n",
    "            preds, trues = strategy.run(self._predict_step, args=(dist_inputs,))\n",
    "            if tf.is_tensor(preds):\n",
    "                return [preds], [trues]\n",
    "            else:\n",
    "                return preds.values, trues.values\n",
    "        \n",
    "        valid_preds_accum = np.zeros([0, 1], dtype=np.float32)\n",
    "        test_preds_accum = np.zeros([0, 1], dtype=np.float32)\n",
    "        \n",
    "        epochs_score = 0.\n",
    "        last_epoch_score = 0.\n",
    "        best_score = 0.\n",
    "        for epoch in range(epochs):\n",
    "            train_dist_ds = strategy.experimental_distribute_dataset(train_ds)\n",
    "            valid_dist_ds = strategy.experimental_distribute_dataset(valid_ds)\n",
    "            test_dist_ds = strategy.experimental_distribute_dataset(test_ds)\n",
    "            train_dist_ds = tqdm.tqdm(train_dist_ds)\n",
    "            valid_dist_ds = tqdm.tqdm(valid_dist_ds)\n",
    "            test_dist_ds = tqdm.tqdm(test_dist_ds)\n",
    "            \n",
    "            # TRAIN LOOP -----------------------------------\n",
    "            epoch_loss = 0.\n",
    "            for i, inputs in enumerate(train_dist_ds):\n",
    "                loss = distributed_train_step(inputs)\n",
    "                epoch_loss += loss\n",
    "                train_dist_ds.set_description(\n",
    "                    \"Scores [{:.4f}, {:.4f}] : Loss {:.5f}\"\n",
    "                    .format(epochs_score, last_epoch_score, epoch_loss/(i+1)))\n",
    "            \n",
    "            # VALIDATION LOOP ------------------------------\n",
    "            valid_trues_accum = np.zeros([0, 1], dtype=np.float32)\n",
    "            for inputs in valid_dist_ds:\n",
    "                preds, trues = distributed_predict_step(inputs)\n",
    "                for pred, true in zip(preds, trues):\n",
    "                    valid_preds_accum = np.concatenate(\n",
    "                        [valid_preds_accum, pred.numpy()], axis=0)\n",
    "                    valid_trues_accum = np.concatenate(\n",
    "                        [valid_trues_accum, true.numpy()], axis=0)\n",
    "                    \n",
    "            last_epoch_score = metrics.roc_auc_score(\n",
    "                valid_trues_accum, valid_preds_accum.reshape((epoch+1, -1))[-1])\n",
    "            if last_epoch_score > best_score:\n",
    "                best_score = last_epoch_score\n",
    "                self.keras_model.save_weights(f'output/weights/model-{fold}-{epoch}.h5')\n",
    "            epochs_score = metrics.roc_auc_score(\n",
    "                valid_trues_accum, np.average(\n",
    "                    valid_preds_accum.reshape((epoch+1, -1)), \n",
    "                    axis=0, weights=[2**i for i in range(epoch+1)]))\n",
    "            \n",
    "            # TEST LOOP ------------------------------------\n",
    "            for inputs in test_dist_ds:\n",
    "                preds, trues = distributed_predict_step(inputs)\n",
    "                for pred, _ in zip(preds, trues):\n",
    "                    test_preds_accum = np.concatenate(\n",
    "                        [test_preds_accum, pred.numpy()], axis=0)\n",
    "                    \n",
    "        return valid_preds_accum, test_preds_accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe49618a8c047368a930c420c2a7676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf04aad77bcf4c08a584adc191fab852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64718bb884184c5d906769375a117c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold = 0\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "learning_rate = 3e-4\n",
    "input_shape=(384, 384, 3)\n",
    "units = [1]\n",
    "dropout = [0.2]\n",
    "activation = [None]\n",
    "label_smoothing = 0.0\n",
    "weights = \"imagenet\"\n",
    "\n",
    "\n",
    "gss = model_selection.GroupShuffleSplit(n_splits=5, random_state=42)\n",
    "gss_iterator = gss.split(\n",
    "    X=train_data.index, \n",
    "    y=train_data.target,\n",
    "    groups=train_data.patient_id\n",
    ")\n",
    "for _ in range(fold+1): \n",
    "    train_idx, valid_idx = next(gss_iterator)\n",
    "    \n",
    "\n",
    "train_ds = create_dataset(\n",
    "    path=input_path+'jpeg/train/', \n",
    "    dataframe=train_data.iloc[train_idx],\n",
    "    input_shape=input_shape,\n",
    "    batch_size=batch_size,\n",
    "    augment=True,\n",
    "    shuffle=True,\n",
    "    cache=False,)\n",
    "\n",
    "valid_ds = create_dataset(\n",
    "    path=input_path+'jpeg/train/', \n",
    "    dataframe=train_data.iloc[valid_idx], \n",
    "    input_shape=input_shape,\n",
    "    batch_size=batch_size,\n",
    "    augment=False,\n",
    "    shuffle=False,\n",
    "    cache=False,)\n",
    "\n",
    "test_ds = create_dataset(\n",
    "    path=input_path+'jpeg/test/', \n",
    "    dataframe=test_data, \n",
    "    input_shape=input_shape,\n",
    "    batch_size=batch_size,\n",
    "    augment=False,\n",
    "    shuffle=False,\n",
    "    cache=False,)\n",
    "\n",
    "strategy = tf.distribute.OneDeviceStrategy(device='GPU')\n",
    "\n",
    "with strategy.scope():\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    \n",
    "    model = NeuralNet(\n",
    "        engine=Engine,\n",
    "        input_shape=input_shape,\n",
    "        units=units,\n",
    "        dropout=dropout,\n",
    "        activation=activation,\n",
    "        weights=weights)\n",
    "    \n",
    "    model.build([None, *input_shape])\n",
    "    \n",
    "    dist_model = DistributedModel(\n",
    "        keras_model=model,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer, \n",
    "        strategy=strategy,\n",
    "        mixed_precision=True, \n",
    "        label_smoothing=label_smoothing)\n",
    "\n",
    "    valid_preds, test_preds = dist_model.fit_and_predict(\n",
    "        fold=fold, \n",
    "        epochs=epochs, \n",
    "        train_ds=train_ds, \n",
    "        valid_ds=valid_ds, \n",
    "        test_ds=test_ds, \n",
    "        strategy=strategy\n",
    "    )\n",
    "    \n",
    "    \n",
    "submission_data.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00757897],\n",
       "       [0.00747694],\n",
       "       [0.0079152 ],\n",
       "       ...,\n",
       "       [0.00238677],\n",
       "       [0.00852828],\n",
       "       [0.00882777]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8573937215145562"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(train_data.iloc[valid_idx].target, valid_preds.reshape((2, -1))[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.898e+03, 6.190e+02, 1.900e+02, 8.800e+01, 4.500e+01, 2.900e+01,\n",
       "        2.200e+01, 1.400e+01, 1.300e+01, 9.000e+00, 6.000e+00, 4.000e+00,\n",
       "        3.000e+00, 7.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 3.000e+00, 1.000e+00, 1.000e+00, 1.000e+00, 2.000e+00,\n",
       "        1.000e+00, 1.000e+00, 3.000e+00, 0.000e+00, 1.000e+00, 0.000e+00,\n",
       "        0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00, 0.000e+00,\n",
       "        0.000e+00, 1.000e+00]),\n",
       " array([5.60798107e-28, 1.19795026e-02, 2.39590053e-02, 3.59385088e-02,\n",
       "        4.79180105e-02, 5.98975122e-02, 7.18770176e-02, 8.38565156e-02,\n",
       "        9.58360210e-02, 1.07815519e-01, 1.19795024e-01, 1.31774530e-01,\n",
       "        1.43754035e-01, 1.55733526e-01, 1.67713031e-01, 1.79692537e-01,\n",
       "        1.91672042e-01, 2.03651547e-01, 2.15631038e-01, 2.27610543e-01,\n",
       "        2.39590049e-01, 2.51569539e-01, 2.63549060e-01, 2.75528550e-01,\n",
       "        2.87508070e-01, 2.99487561e-01, 3.11467052e-01, 3.23446572e-01,\n",
       "        3.35426062e-01, 3.47405583e-01, 3.59385073e-01, 3.71364564e-01,\n",
       "        3.83344084e-01, 3.95323575e-01, 4.07303095e-01, 4.19282585e-01,\n",
       "        4.31262076e-01, 4.43241596e-01, 4.55221087e-01, 4.67200607e-01,\n",
       "        4.79180098e-01, 4.91159588e-01, 5.03139079e-01, 5.15118599e-01,\n",
       "        5.27098119e-01, 5.39077640e-01, 5.51057100e-01, 5.63036621e-01,\n",
       "        5.75016141e-01, 5.86995602e-01, 5.98975122e-01], dtype=float32),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(valid_preds.reshape((2, -1))[-2], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.430e+03, 1.685e+03, 3.680e+02, 1.420e+02, 8.700e+01, 4.600e+01,\n",
       "        4.000e+01, 2.900e+01, 2.700e+01, 2.400e+01, 1.700e+01, 1.500e+01,\n",
       "        1.400e+01, 1.100e+01, 5.000e+00, 0.000e+00, 5.000e+00, 2.000e+00,\n",
       "        5.000e+00, 0.000e+00, 2.000e+00, 1.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 1.000e+00, 1.000e+00, 2.000e+00, 3.000e+00, 1.000e+00,\n",
       "        2.000e+00, 4.000e+00, 0.000e+00, 0.000e+00, 2.000e+00, 2.000e+00,\n",
       "        2.000e+00, 2.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        2.000e+00, 1.000e+00]),\n",
       " array([7.06491270e-27, 9.69844311e-03, 1.93968862e-02, 2.90953293e-02,\n",
       "        3.87937725e-02, 4.84922156e-02, 5.81906587e-02, 6.78891018e-02,\n",
       "        7.75875449e-02, 8.72859880e-02, 9.69844311e-02, 1.06682874e-01,\n",
       "        1.16381317e-01, 1.26079753e-01, 1.35778204e-01, 1.45476639e-01,\n",
       "        1.55175090e-01, 1.64873526e-01, 1.74571976e-01, 1.84270412e-01,\n",
       "        1.93968862e-01, 2.03667298e-01, 2.13365749e-01, 2.23064184e-01,\n",
       "        2.32762635e-01, 2.42461070e-01, 2.52159506e-01, 2.61857957e-01,\n",
       "        2.71556407e-01, 2.81254828e-01, 2.90953279e-01, 3.00651729e-01,\n",
       "        3.10350180e-01, 3.20048600e-01, 3.29747051e-01, 3.39445502e-01,\n",
       "        3.49143952e-01, 3.58842373e-01, 3.68540823e-01, 3.78239274e-01,\n",
       "        3.87937725e-01, 3.97636145e-01, 4.07334596e-01, 4.17033046e-01,\n",
       "        4.26731497e-01, 4.36429918e-01, 4.46128368e-01, 4.55826819e-01,\n",
       "        4.65525270e-01, 4.75223690e-01, 4.84922141e-01], dtype=float32),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(test_preds.reshape((2, -1))[-2], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
